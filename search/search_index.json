{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Profiling Data Management When performing complex profiling, developers often find themselves lost in a maze of repetitive commands and scattered files. You run go test -bench=BenchmarkMyFunc -cpuprofile=cpu.out , then go tool pprof -top cpu.out > results.txt , inspect a function with go tool pprof -list=MyFunc cpu.out , make modifications, run the benchmark again\u2014and hours later, you're exhausted, have dozens of inconsistently named files scattered across directories, and can't remember which changes led to which results. Without systematic organization, you lose track of your optimization journey, lack accurate \"before and after\" snapshots to share with your team, and waste valuable time context-switching between profiling commands instead of focusing on actual performance improvements. Prof eliminates this chaos by capturing everything in one command and automatically organizing all profiling data\u2014binary files, text reports, function-level analysis, and visualizations\u2014into a structured, tagged hierarchy that preserves your optimization history and makes collaboration effortless. Auto The auto command wraps go test and pprof to run benchmarks, collect all profile types, and organize everything automatically: prof auto --benchmarks \"BenchmarkGenPool\" --profiles \"cpu,memory,mutex,block\" --count 10 --tag \"baseline\" This single command replaces dozens of manual steps and creates a complete, organized profiling dataset ready for analysis or comparison. Output Structure: bench/baseline/ \u251c\u2500\u2500 description.txt # User documentation for this run \u251c\u2500\u2500 bin/BenchmarkGenPool/ # Binary profile files \u2502 \u251c\u2500\u2500 BenchmarkGenPool_cpu.out \u2502 \u251c\u2500\u2500 BenchmarkGenPool_memory.out \u2502 \u251c\u2500\u2500 BenchmarkGenPool_mutex.out \u2502 \u2514\u2500\u2500 BenchmarkGenPool_block.out \u251c\u2500\u2500 text/BenchmarkGenPool/ # Text reports & benchmark output \u2502 \u251c\u2500\u2500 BenchmarkGenPool_cpu.txt \u2502 \u251c\u2500\u2500 BenchmarkGenPool_memory.txt \u2502 \u2514\u2500\u2500 BenchmarkGenPool.txt \u251c\u2500\u2500 cpu_functions/BenchmarkGenPool/ # Function-level CPU profile data \u2502 \u251c\u2500\u2500 Put.txt \u2502 \u251c\u2500\u2500 Get.txt \u2502 \u2514\u2500\u2500 getShard.txt \u2514\u2500\u2500 memory_functions/BenchmarkGenPool/ # Function-level memory profile data \u251c\u2500\u2500 Put.txt \u2514\u2500\u2500 allocator.txt Auto - Configuration By default, prof collects all functions shown in the text report of a profile. To customize this behavior, run: prof setup This creates a configuration file with the following structure: { \"function_collection_filter\": { \"BenchmarkGenPool\": { \"include_prefixes\": [\"github.com/example/GenPool\"], \"ignore_functions\": [\"init\", \"TestMain\", \"BenchmarkMain\"] } } } Configuration Options: BenchmarkGenPool : Replace it with your benchmark function name, or with \"*\" to apply for all benchmarks. include_prefixes : Only collect functions whose names start with these prefixes. ignore_functions : Exclude specific functions from collection, even if they match the include prefixes. This filtering helps focus profiling on relevant code paths while excluding test setup and initialization functions that may not be meaningful for performance analysis. Manual The manual command processes existing profile files without running benchmarks - it only uses pprof to organize data you already have: prof manual --tag \"external-profiles\" BenchmarkGenPool_cpu.out memory.out block.out This organizes your existing profile files into a flatter structure based on the profile filename: Manual Output Structure: bench/external-profiles/ \u251c\u2500\u2500 BenchmarkGenPool_cpu/ \u2502 \u251c\u2500\u2500 BenchmarkGenPool_cpu.txt # Text report \u2502 \u2514\u2500\u2500 functions/ # Function-level profile data \u2502 \u251c\u2500\u2500 Put.txt \u2502 \u251c\u2500\u2500 Get.txt \u2502 \u2514\u2500\u2500 getShard.txt \u251c\u2500\u2500 memory/ \u2502 \u251c\u2500\u2500 memory.txt # Text report \u2502 \u2514\u2500\u2500 functions/ # Function-level profile data \u2502 \u2514\u2500\u2500 allocator.txt \u2514\u2500\u2500 block/ \u251c\u2500\u2500 block.txt # Text report \u2514\u2500\u2500 functions/ # Function-level profile data \u2514\u2500\u2500 runtime.txt Manual - Configuration The configuration works the same as auto configuration, except you should use profile file base names (without extensions) instead of benchmark names: { \"function_collection_filter\": { \"BenchmarkGenPool_cpu\": { \"include_prefixes\": [\"github.com/example/GenPool\"], \"ignore_functions\": [\"init\", \"TestMain\", \"BenchmarkMain\"] } } } For example, BenchmarkGenPool_cpu.out becomes BenchmarkGenPool_cpu in the configuration. Performance Comparison Prof's performance comparison automatically drills down from benchmark-level changes to show you exactly which functions changed. Instead of just reporting that performance improved or regressed, Prof pinpoints the specific functions responsible and shows you detailed before-and-after comparisons. Track Auto Use track auto when comparing data collected with prof auto . Simply reference the tag names: prof track auto --base \"baseline\" --current \"optimized\" \\ --profile-type \"cpu\" --bench-name \"BenchmarkGenPool\" \\ --output-format \"summary\" prof track auto --base \"baseline\" --current \"optimized\" \\ --profile-type \"cpu\" --bench-name \"BenchmarkGenPool\" \\ --output-format \"detailed\" Track Manual Use track manual when comparing external profile files by specifying their relative paths: prof track manual --base path/to/base/report/cpu.txt \\ --current path/to/current/report/cpu.txt \\ --output-format \"summary\" prof track manual --base path/to/base/report/cpu.txt \\ --current path/to/current/report/cpu.txt \\ --output-format \"detailed\" Output Formats Prof's performance comparison provides multiple output formats to help you understand performance changes at different levels of detail and presentation. Currently supported formats: Terminal (default) HTML JSON Summary Format The summary format gives you a high-level overview of all performance changes, organized by impact: ==== Performance Tracking Summary ==== Total Functions Analyzed: 78 Regressions: 9 Improvements: 8 Stable: 61 \u26a0\ufe0f Top Regressions (worst first): \u2022 internal/cache.getShard: +200.0% (0.030s \u2192 0.090s) \u2022 internal/hash.Spread: +180.0% (0.050s \u2192 0.140s) \u2022 pool/acquire: +150.0% (0.020s \u2192 0.050s) \u2022 encoding/json.Marshal: +125.0% (0.080s \u2192 0.180s) \u2022 sync.Pool.Get: +100.0% (0.010s \u2192 0.020s) \u2705 Top Improvements (best first): \u2022 compress/gzip.NewWriter: -100.0% (0.020s \u2192 0.000s) \u2022 internal/metrics.resetCounters: -100.0% (0.010s \u2192 0.000s) \u2022 encoding/json.Unmarshal: -95.0% (0.100s \u2192 0.005s) \u2022 net/url.ParseQuery: -90.0% (0.050s \u2192 0.005s) \u2022 pool/isFull: -85.0% (0.020s \u2192 0.003s) Detailed Format The detailed format provides comprehensive analysis for each changed function, including impact assessment and action recommendations: \ud83d\udcca Summary: 78 total functions | \ud83d\udd34 9 regressions | \ud83d\udfe2 8 improvements | \u26aa 61 stable \ud83d\udccb Report Order: Regressions first (worst \u2192 best), then Improvements (best \u2192 worst), then Stable \u2551 \u2551 \u2551 \u2551 \u2551 \u2551 \u2551 PERFORMANCE CHANGE REPORT Function: github.com/Random/Pool/pool.getShard Analysis Time: 2025-07-23 15:51:59 PDT Change Type: REGRESSION \u26a0\ufe0f Performance regression detected \u2551 \u2551 \u2551 \u2551 \u2551 FLAT TIME ANALYSIS Before: 0.030000s After: 0.090000s Delta: +0.060000s Change: +200.00% Impact: Function is 200.00% SLOWER \u2551 \u2551 \u2551 \u2551 \u2551 CUMULATIVE TIME ANALYSIS Before: 0.030s After: 0.100s Delta: +0.070s Change: +233.33% \u2551 \u2551 \u2551 \u2551 \u2551 IMPACT ASSESSMENT Severity: CRITICAL Recommendation: Critical regression! Immediate investigation required. HTML & JSON Output In addition to terminal display, Prof can export both summary and detailed reports in: \ud83d\udcc4 HTML : shareable and human-friendly \ud83e\udde9 JSON : structured format for programmatic use or further integration --output-format summary-html --output-format detailed-json CI/CD: Fail on regressions Understanding the regression threshold: The --regression-threshold flag sets a percentage limit on performance regressions. When enabled with --fail-on-regression , the command will exit with a non-zero status code if any function's flat time regression exceeds this threshold. Flat time regression calculation: Flat regression % = (current_time - baseline_time) / baseline_time \u00d7 100 Example: If a function took 100ms in baseline and 110ms in current run: Flat regression = (110 - 100) / 100 \u00d7 100 = +10% With --regression-threshold 5.0 , this would fail the build With --regression-threshold 15.0 , this would pass Note: The threshold applies to flat time (time spent directly in the function), not cumulative time (time including all called functions). Flat time gives a more direct measure of the function's own performance impact. Important: The prof command must be run from within the Go project directory where the benchmarks are located, otherwise it will fail with \"go: cannot find main module\" errors. This means running prof from the exact directory containing your *_test.go files with the benchmarks. prof track auto \\ --base baseline \\ --current PR \\ --profile-type cpu \\ --bench-name BenchmarkGenPool \\ --output-format summary \\ --fail-on-regression \\ --regression-threshold 5.0 Example GitHub Actions job: name: perf-regression-check on: [pull_request] jobs: check: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-go@v5 with: go-version: \">=1.24\" - name: Install prof run: go install github.com/AlexsanderHamir/prof/cmd/prof@latest - name: Collect baseline (main) run: | git fetch origin main --depth=1 git checkout -qf origin/main # prof must be run from within the Go project directory where benchmarks are located cd ${{ github.workspace }} prof auto --benchmarks \"BenchmarkGenPool\" --profiles \"cpu\" --count 5 --tag baseline - name: Collect current (PR) run: | git checkout - # prof must be run from within the Go project directory where benchmarks are located cd ${{ github.workspace }} prof auto --benchmarks \"BenchmarkGenPool\" --profiles \"cpu\" --count 5 --tag PR - name: Compare and fail on regression run: | # prof must be run from within the Go project directory where benchmarks are located cd ${{ github.workspace }} prof track auto --base baseline --current PR \\ --profile-type cpu --bench-name \"BenchmarkGenPool\" \\ --output-format summary --fail-on-regression --regression-threshold 5.0","title":"Profiling Data Management"},{"location":"#profiling-data-management","text":"When performing complex profiling, developers often find themselves lost in a maze of repetitive commands and scattered files. You run go test -bench=BenchmarkMyFunc -cpuprofile=cpu.out , then go tool pprof -top cpu.out > results.txt , inspect a function with go tool pprof -list=MyFunc cpu.out , make modifications, run the benchmark again\u2014and hours later, you're exhausted, have dozens of inconsistently named files scattered across directories, and can't remember which changes led to which results. Without systematic organization, you lose track of your optimization journey, lack accurate \"before and after\" snapshots to share with your team, and waste valuable time context-switching between profiling commands instead of focusing on actual performance improvements. Prof eliminates this chaos by capturing everything in one command and automatically organizing all profiling data\u2014binary files, text reports, function-level analysis, and visualizations\u2014into a structured, tagged hierarchy that preserves your optimization history and makes collaboration effortless.","title":"Profiling Data Management"},{"location":"#auto","text":"The auto command wraps go test and pprof to run benchmarks, collect all profile types, and organize everything automatically: prof auto --benchmarks \"BenchmarkGenPool\" --profiles \"cpu,memory,mutex,block\" --count 10 --tag \"baseline\" This single command replaces dozens of manual steps and creates a complete, organized profiling dataset ready for analysis or comparison. Output Structure: bench/baseline/ \u251c\u2500\u2500 description.txt # User documentation for this run \u251c\u2500\u2500 bin/BenchmarkGenPool/ # Binary profile files \u2502 \u251c\u2500\u2500 BenchmarkGenPool_cpu.out \u2502 \u251c\u2500\u2500 BenchmarkGenPool_memory.out \u2502 \u251c\u2500\u2500 BenchmarkGenPool_mutex.out \u2502 \u2514\u2500\u2500 BenchmarkGenPool_block.out \u251c\u2500\u2500 text/BenchmarkGenPool/ # Text reports & benchmark output \u2502 \u251c\u2500\u2500 BenchmarkGenPool_cpu.txt \u2502 \u251c\u2500\u2500 BenchmarkGenPool_memory.txt \u2502 \u2514\u2500\u2500 BenchmarkGenPool.txt \u251c\u2500\u2500 cpu_functions/BenchmarkGenPool/ # Function-level CPU profile data \u2502 \u251c\u2500\u2500 Put.txt \u2502 \u251c\u2500\u2500 Get.txt \u2502 \u2514\u2500\u2500 getShard.txt \u2514\u2500\u2500 memory_functions/BenchmarkGenPool/ # Function-level memory profile data \u251c\u2500\u2500 Put.txt \u2514\u2500\u2500 allocator.txt","title":"Auto"},{"location":"#auto-configuration","text":"By default, prof collects all functions shown in the text report of a profile. To customize this behavior, run: prof setup This creates a configuration file with the following structure: { \"function_collection_filter\": { \"BenchmarkGenPool\": { \"include_prefixes\": [\"github.com/example/GenPool\"], \"ignore_functions\": [\"init\", \"TestMain\", \"BenchmarkMain\"] } } } Configuration Options: BenchmarkGenPool : Replace it with your benchmark function name, or with \"*\" to apply for all benchmarks. include_prefixes : Only collect functions whose names start with these prefixes. ignore_functions : Exclude specific functions from collection, even if they match the include prefixes. This filtering helps focus profiling on relevant code paths while excluding test setup and initialization functions that may not be meaningful for performance analysis.","title":"Auto - Configuration"},{"location":"#manual","text":"The manual command processes existing profile files without running benchmarks - it only uses pprof to organize data you already have: prof manual --tag \"external-profiles\" BenchmarkGenPool_cpu.out memory.out block.out This organizes your existing profile files into a flatter structure based on the profile filename: Manual Output Structure: bench/external-profiles/ \u251c\u2500\u2500 BenchmarkGenPool_cpu/ \u2502 \u251c\u2500\u2500 BenchmarkGenPool_cpu.txt # Text report \u2502 \u2514\u2500\u2500 functions/ # Function-level profile data \u2502 \u251c\u2500\u2500 Put.txt \u2502 \u251c\u2500\u2500 Get.txt \u2502 \u2514\u2500\u2500 getShard.txt \u251c\u2500\u2500 memory/ \u2502 \u251c\u2500\u2500 memory.txt # Text report \u2502 \u2514\u2500\u2500 functions/ # Function-level profile data \u2502 \u2514\u2500\u2500 allocator.txt \u2514\u2500\u2500 block/ \u251c\u2500\u2500 block.txt # Text report \u2514\u2500\u2500 functions/ # Function-level profile data \u2514\u2500\u2500 runtime.txt","title":"Manual"},{"location":"#manual-configuration","text":"The configuration works the same as auto configuration, except you should use profile file base names (without extensions) instead of benchmark names: { \"function_collection_filter\": { \"BenchmarkGenPool_cpu\": { \"include_prefixes\": [\"github.com/example/GenPool\"], \"ignore_functions\": [\"init\", \"TestMain\", \"BenchmarkMain\"] } } } For example, BenchmarkGenPool_cpu.out becomes BenchmarkGenPool_cpu in the configuration.","title":"Manual - Configuration"},{"location":"#performance-comparison","text":"Prof's performance comparison automatically drills down from benchmark-level changes to show you exactly which functions changed. Instead of just reporting that performance improved or regressed, Prof pinpoints the specific functions responsible and shows you detailed before-and-after comparisons.","title":"Performance Comparison"},{"location":"#track-auto","text":"Use track auto when comparing data collected with prof auto . Simply reference the tag names: prof track auto --base \"baseline\" --current \"optimized\" \\ --profile-type \"cpu\" --bench-name \"BenchmarkGenPool\" \\ --output-format \"summary\" prof track auto --base \"baseline\" --current \"optimized\" \\ --profile-type \"cpu\" --bench-name \"BenchmarkGenPool\" \\ --output-format \"detailed\"","title":"Track Auto"},{"location":"#track-manual","text":"Use track manual when comparing external profile files by specifying their relative paths: prof track manual --base path/to/base/report/cpu.txt \\ --current path/to/current/report/cpu.txt \\ --output-format \"summary\" prof track manual --base path/to/base/report/cpu.txt \\ --current path/to/current/report/cpu.txt \\ --output-format \"detailed\"","title":"Track Manual"},{"location":"#output-formats","text":"Prof's performance comparison provides multiple output formats to help you understand performance changes at different levels of detail and presentation. Currently supported formats: Terminal (default) HTML JSON","title":"Output Formats"},{"location":"#summary-format","text":"The summary format gives you a high-level overview of all performance changes, organized by impact: ==== Performance Tracking Summary ==== Total Functions Analyzed: 78 Regressions: 9 Improvements: 8 Stable: 61 \u26a0\ufe0f Top Regressions (worst first): \u2022 internal/cache.getShard: +200.0% (0.030s \u2192 0.090s) \u2022 internal/hash.Spread: +180.0% (0.050s \u2192 0.140s) \u2022 pool/acquire: +150.0% (0.020s \u2192 0.050s) \u2022 encoding/json.Marshal: +125.0% (0.080s \u2192 0.180s) \u2022 sync.Pool.Get: +100.0% (0.010s \u2192 0.020s) \u2705 Top Improvements (best first): \u2022 compress/gzip.NewWriter: -100.0% (0.020s \u2192 0.000s) \u2022 internal/metrics.resetCounters: -100.0% (0.010s \u2192 0.000s) \u2022 encoding/json.Unmarshal: -95.0% (0.100s \u2192 0.005s) \u2022 net/url.ParseQuery: -90.0% (0.050s \u2192 0.005s) \u2022 pool/isFull: -85.0% (0.020s \u2192 0.003s)","title":"Summary Format"},{"location":"#detailed-format","text":"The detailed format provides comprehensive analysis for each changed function, including impact assessment and action recommendations: \ud83d\udcca Summary: 78 total functions | \ud83d\udd34 9 regressions | \ud83d\udfe2 8 improvements | \u26aa 61 stable \ud83d\udccb Report Order: Regressions first (worst \u2192 best), then Improvements (best \u2192 worst), then Stable \u2551 \u2551 \u2551 \u2551 \u2551 \u2551 \u2551 PERFORMANCE CHANGE REPORT Function: github.com/Random/Pool/pool.getShard Analysis Time: 2025-07-23 15:51:59 PDT Change Type: REGRESSION \u26a0\ufe0f Performance regression detected \u2551 \u2551 \u2551 \u2551 \u2551 FLAT TIME ANALYSIS Before: 0.030000s After: 0.090000s Delta: +0.060000s Change: +200.00% Impact: Function is 200.00% SLOWER \u2551 \u2551 \u2551 \u2551 \u2551 CUMULATIVE TIME ANALYSIS Before: 0.030s After: 0.100s Delta: +0.070s Change: +233.33% \u2551 \u2551 \u2551 \u2551 \u2551 IMPACT ASSESSMENT Severity: CRITICAL Recommendation: Critical regression! Immediate investigation required.","title":"Detailed Format"},{"location":"#html-json-output","text":"In addition to terminal display, Prof can export both summary and detailed reports in: \ud83d\udcc4 HTML : shareable and human-friendly \ud83e\udde9 JSON : structured format for programmatic use or further integration --output-format summary-html --output-format detailed-json","title":"HTML &amp; JSON Output"},{"location":"#cicd-fail-on-regressions","text":"Understanding the regression threshold: The --regression-threshold flag sets a percentage limit on performance regressions. When enabled with --fail-on-regression , the command will exit with a non-zero status code if any function's flat time regression exceeds this threshold. Flat time regression calculation: Flat regression % = (current_time - baseline_time) / baseline_time \u00d7 100 Example: If a function took 100ms in baseline and 110ms in current run: Flat regression = (110 - 100) / 100 \u00d7 100 = +10% With --regression-threshold 5.0 , this would fail the build With --regression-threshold 15.0 , this would pass Note: The threshold applies to flat time (time spent directly in the function), not cumulative time (time including all called functions). Flat time gives a more direct measure of the function's own performance impact. Important: The prof command must be run from within the Go project directory where the benchmarks are located, otherwise it will fail with \"go: cannot find main module\" errors. This means running prof from the exact directory containing your *_test.go files with the benchmarks. prof track auto \\ --base baseline \\ --current PR \\ --profile-type cpu \\ --bench-name BenchmarkGenPool \\ --output-format summary \\ --fail-on-regression \\ --regression-threshold 5.0 Example GitHub Actions job: name: perf-regression-check on: [pull_request] jobs: check: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-go@v5 with: go-version: \">=1.24\" - name: Install prof run: go install github.com/AlexsanderHamir/prof/cmd/prof@latest - name: Collect baseline (main) run: | git fetch origin main --depth=1 git checkout -qf origin/main # prof must be run from within the Go project directory where benchmarks are located cd ${{ github.workspace }} prof auto --benchmarks \"BenchmarkGenPool\" --profiles \"cpu\" --count 5 --tag baseline - name: Collect current (PR) run: | git checkout - # prof must be run from within the Go project directory where benchmarks are located cd ${{ github.workspace }} prof auto --benchmarks \"BenchmarkGenPool\" --profiles \"cpu\" --count 5 --tag PR - name: Compare and fail on regression run: | # prof must be run from within the Go project directory where benchmarks are located cd ${{ github.workspace }} prof track auto --base baseline --current PR \\ --profile-type cpu --bench-name \"BenchmarkGenPool\" \\ --output-format summary --fail-on-regression --regression-threshold 5.0","title":"CI/CD: Fail on regressions"}]}